'''
FileName : crawler.py 
#2025-12-26 : ì •ì¤€ì˜
'''
import requests # ì›¹í˜ì´ì§€ ì ‘ì† ì²˜ë¦¬ë¥¼ ìœ„í•œ request import
from requests.exceptions import RequestException # RequestException ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„ ì–¸
from bs4 import BeautifulSoup # BeatifulSoup ëª¨ë“ˆ import https://pypi.org/project/beautifulsoup4/ ì°¸ê³ 
from sqlalchemy.orm import Session # Python ORM ì‚¬ìš©ì„ ìœ„í•œ sqlalchemy import
from app import models
from app.database import SessionLocal, engine
from app.models import CrawledArticle

def crawl_fashion_breaking_news():
    """
    Need: í•˜ë“œì½”ë”©ëœ urlì— ì ‘ì†í•˜ì—¬ í¬ë¡¤ë§.

    Args:

    Returns:
        í¬ë¡¤ë§ ìš”ì†Œë“¤
    """

    models.Base.metadata.create_all(bind=engine)
    # model íŒŒì¼ì— ì •ì˜ëœ table ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ engineì— ì—°ê²°ëœ DB í…Œì´ë¸” ìœ /ë¬´ì— ë”°ë¥¸ ìƒì„±
    # DB í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±

    print("ğŸš€ [íŒ¨ì…˜/ë·°í‹°] ì‹¤ì‹œê°„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")


    # ìš”ì²­ì •ë³´ ë””í´íŠ¸ ì„¸íŒ…
    #url = "https://news.naver.com/breakingnews/section/103/376"
    url = "í¬ë¡¤ë§í•  url"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language" : "ko-KR,ko;q=0.9,en;q=0.8",
        "Referer": "https://news.naver.com/"
    }
    # User-Agent : **â€œì´ ìš”ì²­ì„ ë³´ë‚¸ í´ë¼ì´ì–¸íŠ¸ê°€ ëˆ„êµ¬ì¸ì§€(ë¸Œë¼ìš°ì €/ì•±/ë´‡ ë“±)â€**ë¥¼ ì„œë²„ì— ì•Œë ¤ì£¼ëŠ” ë¬¸ìì—´.
    # Accept : í´ë¼ì´ì–¸íŠ¸ê°€ ì–´ë–¤ MIME íƒ€ì…ì„ ë°›ì„ ìˆ˜ ìˆëŠ”ì§€ ì„œë²„ì— ì „ë‹¬

    # request ì‹¤í–‰
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
    except RequestException as exc:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {exc}")
        return

    soup = BeautifulSoup(response.text, "html.parser")
    
    list_selectors = [
        "ul.sa_list_news > li.sa_item",
        "div.sa_list_news > ul > li.sa_item",
        "div.sa_list > ul > li.sa_item",
        "div.sa_list .sa_item",
        "li.sa_item",
        ".sa_item",
    ]

    matched_selector = None
    news_items = []
    for selector in list_selectors:
        news_items = soup.select(selector)
        if news_items:
            matched_selector = selector
            break

    print(f"ğŸ” ë§¤ì¹­ëœ selector: {matched_selector} / items: {len(news_items)}")

    if not news_items:
        print(f"âš ï¸ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (HTML ê¸¸ì´: {len(response.text)})")
        return

    print(f"âœ… ê¸°ì‚¬ ì•„ì´í…œ {len(news_items)}ê°œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. DB ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    db: Session = SessionLocal()
    count = 0

    try:
        for item in news_items:
            # ì œëª© ë° ë§í¬ ì¶”ì¶œ
            title_tag = item.select_one(".sa_text_title")
            if not title_tag:
                title_tag = item.select_one("a[href*='/article/']")
            
            if not title_tag:
                continue

            if title_tag.name == 'a':
                link = title_tag['href']
            else:
                parent_a = title_tag.find_parent('a')
                link = parent_a['href'] if parent_a else ""

            title = title_tag.get_text(strip=True)

            if link and link.startswith("/"):
                link = f"https://news.naver.com{link}"

            if not link:
                continue

            summary_tag = item.select_one(".sa_text_lede")
            summary = summary_tag.get_text(strip=True) if summary_tag else ""

            # source_tagëŠ” ëª¨ë¸ì— ì €ì¥í•  ê³³ì´ ì—†ìœ¼ë¯€ë¡œ ì¶”ì¶œë§Œ í•˜ê³  ì €ì¥ì€ ì•ˆí•¨
            # source_tag = item.select_one(".sa_text_press")
            # source = source_tag.get_text(strip=True) if source_tag else "Unknown"

            # 26ì¼ ìˆ˜ì • original_url -> link ë¡œ ë³€ê²½
            exists = db.query(CrawledArticle).filter(CrawledArticle.link == link).first()
            if exists:
                continue

            # 26ì¼ ìˆ˜ì • ëª¨ë¸ì— ìˆëŠ” ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•˜ì—¬ ORM ì²˜ë¦¬
            new_article = CrawledArticle(
                link=link,          # original_url -> link
                title=title,
                summary=summary,
                content="",         # í˜„ì¬ ë¦¬ìŠ¤íŠ¸ì—ì„œëŠ” ë³¸ë¬¸ì´ ì—†ìœ¼ë¯€ë¡œ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
                status="PENDING"
            )

            db.add(new_article)
            count += 1
            print(f"  - ì €ì¥: {title[:20]}...")

        db.commit() # í•œë²ˆì— ì»¤ë°‹
        print(f"\nğŸ‰ ì´ {count}ê°œì˜ ê¸°ì‚¬ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e} rollback ì²˜ë¦¬")
        import traceback
        traceback.print_exc()
        db.rollback()
    finally:
        db.close()

if __name__ == "__main__":
    crawl_fashion_breaking_news() # í•¨ìˆ˜ í˜¸ì¶œ

#  Flow
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ crawl_fashion_breaking_news() í˜¸ì¶œ        â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                  â”‚
#                  â–¼
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 1) í…Œì´ë¸” ìƒì„± ì•ˆì „ì¥ì¹˜                   â”‚
# â”‚    models.Base.metadata.create_all(...)   â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                  â”‚
#                  â–¼
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 2) ë„¤ì´ë²„ ì†ë³´ URL / headers ì¤€ë¹„         â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                  â”‚
#                  â–¼
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 3) HTTP ìš”ì²­                              â”‚
# â”‚    response = requests.get(..., timeout)  â”‚
# â”‚    response.raise_for_status()            â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#         â”‚ì„±ê³µ                         â”‚ì‹¤íŒ¨(RequestException)
#         â–¼                             â–¼
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 4) BeautifulSoup íŒŒì‹±       â”‚   â”‚ "ìš”ì²­ ì‹¤íŒ¨" ì¶œë ¥ í›„ returnâ”‚
# â”‚    soup = BeautifulSoup(...)â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#         â”‚
#         â–¼
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 5) ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì„ íƒì íƒìƒ‰                â”‚
# â”‚    for selector in list_selectors:        â”‚
# â”‚        news_items = soup.select(selector) â”‚
# â”‚        ìˆìœ¼ë©´ break                        â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#         â”‚ìˆìŒ                         â”‚ì—†ìŒ
#         â–¼                             â–¼
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 6) DB ì„¸ì…˜ ìƒì„±             â”‚   â”‚ "ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ëª» ì°¾ìŒ" ì¶œë ¥â”‚
# â”‚    db = SessionLocal()      â”‚   â”‚ í›„ return                 â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#         â”‚
#         â–¼
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 7) ê¸°ì‚¬ ì•„ì´í…œ ë£¨í”„                        â”‚
# â”‚    for item in news_items:                â”‚
# â”‚      - ì œëª©/ë§í¬ ì¶”ì¶œ                      â”‚
# â”‚      - ë§í¬ ì •ê·œí™”                         â”‚
# â”‚      - ìš”ì•½ ì¶”ì¶œ                           â”‚
# â”‚      - ì¤‘ë³µ ì²´í¬(select)                   â”‚
# â”‚      - ìƒˆ ê°ì²´ ìƒì„± + db.add()             â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#         â”‚
#         â–¼
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 8) db.commit()                            â”‚
# â”‚    "ì´ Nê°œ ì €ì¥" ì¶œë ¥                     â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#         â”‚
#         â–¼
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 9) finally: db.close()                    â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜